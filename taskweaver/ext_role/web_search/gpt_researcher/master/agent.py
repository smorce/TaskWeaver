import asyncio
import time

from gpt_researcher.config import Config
from gpt_researcher.context.compression import ContextCompressor
from gpt_researcher.master.functions import *
from gpt_researcher.memory import Memory
from gpt_researcher.utils.enum import ReportType

"""ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæ¡ˆ:
ãƒ»scrape_sites_by_query ãƒ¡ã‚½ãƒƒãƒ‰
search_results["body"] ã¯ ãƒšã‚¬ã‚µã‚¹ ã§å–å¾—ã—ãŸ ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ã«ãªã£ã¦ã„ã‚‹ã®ã§ã€ã“ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å…¨ã¦ Gemini Flash ã«å…¥ã‚Œã¦ã€ã‚µãƒ–ã‚¯ã‚¨ãƒªã«é–¢ä¿‚ã®ã‚ã‚Šãã†ãªå†…å®¹ã‚’è¦ç´„ã¨ã—ã¦ä½œæˆã—ã¦ã‚‚ã‚‰ã†ã€‚ãªã‘ã‚Œã°Noneã‚’è¿”ã—ã¦ã‚‚ã‚‰ã†ã€‚
ã“ã†ã™ã‚Œã°ã€åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã‚ãšã«å…¨éƒ¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ©ç”¨ã§ãã‚‹ã€‚
é«˜ç²¾åº¦ãƒ»ä½ã‚³ã‚¹ãƒˆãƒ»é«˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ ã® Gemini Flash ã ã‹ã‚‰ã§ãã‚‹æ¡ˆã€‚

# Gemini Flash ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
=============================================
### æŒ‡ç¤º
Contextã‚’è¸ã¾ãˆã¦Sub Queryã«å¯¾ã—ã¦ã®è¦ç´„ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªã“ã¨ã¯è¨€ã‚ãšã«ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã ã‘ã‚’ä½¿ã£ã¦è¦ç´„ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã€Contextã¨Sub Queryã«é–¢é€£ãŒãªã‘ã‚Œã° None ã¨å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

### Sub Query
{ã‚ã‚ã‚}

### Context
{search_results["body"]}  â†  [ãƒ¡ãƒ¢]å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½¿ç”¨

ç†è§£ã§ããŸã‚‰é–‹å§‹ã—ã¦ãã ã•ã„ï¼
=============================================

search_results["body"] ã¯è¤‡æ•°ã‚ã‚‹ã¯ãšãªã®ã§ã€Foræ–‡ã‚’å›ã—ã¦è¤‡æ•°ã®è¦ç´„ã‚’ç”Ÿæˆã—ã¦ã€ç”Ÿæˆã—ãŸå…¨ã¦ã®è¦ç´„ã‚’æ”¹è¡Œã§é€£çµã•ã›ã¦1ã¤ã®æ–‡å­—åˆ—(all_content)ã«ã™ã‚‹ã€‚
scrape_sites_by_query ãƒ¡ã‚½ãƒƒãƒ‰ã‹ã‚‰è¿”ã£ã¦ããŸã‚‚ã®ãŒ all_content ã€‚
ãã—ã¦ã€ process_sub_query ãƒ¡ã‚½ãƒƒãƒ‰ã® get_similar_content_by_query ã¯ä½¿ã‚ãªã„ã‚ˆã†ã«ã—ã¦ã€ä»£ã‚ã‚Šã« process_sub_query ã§ã¯ all_content ã‚’ return ã•ã›ã‚‹ã€‚
"""


class GPTResearcher:
    """
    GPT Researcher
    """

    def __init__(
        self,
        query: str,
        report_type: str = ReportType.ResearchReport.value,
        source_urls=None,
        config_path=None,
        websocket=None,
        agent=None,
        role=None,
        parent_query: str = "",
        subtopics: list = [],
        visited_urls: set = set(),
        verbose: bool = True,
    ):
        """
        Initialize the GPT Researcher class.
        Args:
            query: str,
            report_type: str
            source_urls
            config_path
            websocket
            agent
            role
            parent_query: str
            subtopics: list
            visited_urls: set
        """
        self.query = query
        self.report_type = report_type
        self.agent = agent
        self.role = role
        self.report_prompt = get_prompt_by_report_type(self.report_type)  # this validates the report type
        self.websocket = websocket
        self.cfg = Config(config_path)  # config_path ã¯ None ã§ã‚‚å•é¡Œãªã„ã€‚config ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« config.json ãŒã‚ã‚Œã°ä¸Šæ›¸ãã•ã‚Œã‚‹ã€‚
        self.retriever = get_retriever(self.cfg.retriever)   # tavily
        self.context = []
        self.source_urls = source_urls
        self.memory = Memory(self.cfg.embedding_provider)
        self.visited_urls = visited_urls
        self.verbose = verbose

        # Only relevant for DETAILED REPORTS
        # --------------------------------------

        # Stores the main query of the detailed report
        self.parent_query = parent_query

        # Stores all the user provided subtopics
        self.subtopics = subtopics

    async def conduct_research(self):
        """
        Runs the GPT Researcher to conduct research
        """
        if self.verbose:
            await stream_output("logs", f"ğŸ” Starting the research task for '{self.query}'...", self.websocket)

        # Generate Agent
        if not (self.agent and self.role):
            print("ãƒ‡ãƒãƒƒã‚° choose_agent å‰")
            self.agent, self.role = await choose_agent(self.query, self.cfg, self.parent_query)
            print("ãƒ‡ãƒãƒƒã‚° self.agent", self.agent)
            print("ãƒ‡ãƒãƒƒã‚° self.role", self.role)

        if self.verbose:
            print("ãƒ‡ãƒãƒƒã‚° stream_output å‰")
            # ã“ã®é–¢æ•°ã®ç›®çš„ã¯ã€å‡ºåŠ›ã‚’WebSocketã‚’é€šã˜ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒ ã™ã‚‹ã“ã¨ã§ã™ã€‚ã¾ãŸã€æ¡ä»¶ã«å¿œã˜ã¦ãƒ­ã‚°ã«å‡ºåŠ›ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
            # websocket ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã® send_json ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€type ã¨ output ã‚’å«ã‚€JSONãƒ‡ãƒ¼ã‚¿ã‚’éåŒæœŸã§é€ä¿¡ã—ã¾ã™ã€‚WebSocketé€šä¿¡ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿã—ã¾ã™ã€‚
            await stream_output("logs", self.agent, self.websocket)
            print("ãƒ‡ãƒãƒƒã‚° stream_output done!")    # ã“ã“ã¯ç¢ºèªã§ããŸ

        # If specified, the researcher will use the given urls as the context for the research.
        if self.source_urls:
            print("ãƒ‡ãƒãƒƒã‚°1 source_urls YES")    # ã“ã£ã¡ã¯ãªã„ã£ã½ã„
            self.context = await self.get_context_by_urls(self.source_urls)
            print("ãƒ‡ãƒãƒƒã‚°1 source_urls done!")
        else:
            print("ãƒ‡ãƒãƒƒã‚°2 source_urls NO")    # ã“ã“ã¯ç¢ºèªã§ããŸ
            self.context = await self.get_context_by_search(self.query)
            print("ãƒ‡ãƒãƒƒã‚°2 source_urls done!")    # ã“ã“ã¯ç¢ºèªã§ããŸ

        time.sleep(2)

        return self.context

    async def write_report(self, existing_headers: list = []):
        """
        Writes the report based on research conducted

        Returns:
            str: The report
        """
        if self.verbose:
            await stream_output("logs", f"âœï¸ Writing summary for research task: {self.query}...", self.websocket)   # ã“ã“ã¯ç¢ºèªã§ããŸ

        if self.report_type == "custom_report":
            self.role = self.cfg.agent_role if self.cfg.agent_role else self.role
            # æœ€æ–°ç‰ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚³ã‚³ã‚‰ã¸ã‚“ã«å¤‰æ›´ãŒå…¥ã£ã¦ã„ã‚‹
        elif self.report_type == "subtopic_report":
            report = await generate_report(
                query=self.query,
                context=self.context,
                agent_role_prompt=self.role,
                report_type=self.report_type,
                websocket=self.websocket,
                cfg=self.cfg,
                main_topic=self.parent_query,
                existing_headers=existing_headers
            )
        else:
            report = await generate_report(
                query=self.query,              # â˜… åˆæœŸè¨ˆç”»ã¨ã‚µãƒ–ã‚¯ã‚¨ãƒªãŒä¸€è‡´ã—ãªã‘ã‚Œã°ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã¨ã—ã¦ self.sub_queries ã‚’ä½œæˆã—ã¦ get_context_by_search ãƒ¡ã‚½ãƒƒãƒ‰ã§ sub_queries ã‚’ self.sub_queries ã«å¤‰æ›´ã— self.query ã®ä»£ã‚ã‚Šã« self.sub_queries ã‚’æ¸¡ã›ã°è‰¯ã„ã€‚self.sub_queries ã®ä¸­ã«ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚‚å…¥ã£ã¦ã„ã‚‹ã‹ã‚‰ã€‚ â†’ å¤šåˆ†ã€ãªã‚‹ã¹ãå…ƒã®ã‚¯ã‚¨ãƒªã«æ²¿ã£ãŸãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ãŸã„ã€ã‚µãƒ–ã‚¯ã‚¨ãƒªã®å½±éŸ¿åŠ›ã‚’ä¸‹ã’ãŸã„ã¨ã„ã†æ„å›³ã§ã€ã“ã“ã¯ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã—ã‹æ¸¡ã—ã¦ã„ãªã„æ°—ãŒã™ã‚‹ã€‚ä¸€æ—¦ã€ã“ã®ã¾ã¾ã§ã€‚
                context=self.context,          # ã“ã‚Œã¯ãƒªã‚¹ãƒˆ
                agent_role_prompt=self.role,
                report_type=self.report_type,
                websocket=self.websocket,
                cfg=self.cfg
            )

        return report

    async def get_context_by_urls(self, urls):
        """
            Scrapes and compresses the context from the given urls
        """
        new_search_urls = await self.get_new_urls(urls)
        if self.verbose:
            await stream_output("logs",
                            f"ğŸ§  I will conduct my research based on the following urls: {new_search_urls}...",
                            self.websocket)
        scraped_sites = scrape_urls(new_search_urls, self.cfg)
        return await self.get_similar_content_by_query(self.query, scraped_sites)

    async def get_context_by_search(self, query):
        """
           Generates the context for the research task by searching the query and scraping the results
        Returns:
            context: List of context
        """
        context = []
        print("ãƒ‡ãƒãƒƒã‚° get_context_by_searché–¢æ•° å‰1")
        # Generate Sub-Queries including original query
        sub_queries = await get_sub_queries(query, self.role, self.cfg, self.parent_query, self.report_type)
        print("ãƒ‡ãƒãƒƒã‚° get_context_by_searché–¢æ•° å¾Œ1")

        print("ãƒ‡ãƒãƒƒã‚° get_context_by_searché–¢æ•° å‰2")
        # If this is not part of a sub researcher, add original query to research for better results
        if self.report_type != "subtopic_report":
            sub_queries.append(query)

        if self.verbose:
            await stream_output("logs",
                                f"ğŸ§  I will conduct my research based on the following queries: {sub_queries}...",
                                self.websocket)
        print("ãƒ‡ãƒãƒƒã‚° get_context_by_searché–¢æ•° å¾Œ2")

        print("ãƒ‡ãƒãƒƒã‚° get_context_by_searché–¢æ•° å‰3")
        # Using asyncio.gather to process the sub_queries asynchronously
        # context(list) ã¨ã¯ TAVILY.search(sub_query) ã‚’ä½¿ã£ã¦ URL ã‚’å–å¾— â†’ BeautifulSoup ã‚’ä½¿ã£ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªæ–‡å­—åˆ—(Webãƒšãƒ¼ã‚¸ã®ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ)ã‚’å–å¾—ã—ã€ãã“ã‹ã‚‰ã‚µãƒ–ã‚¯ã‚¨ãƒªã«é–¢ä¿‚ã®ã‚ã‚Šãã†ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠœãå‡ºã—ãŸã‚‚ã®
        context = await asyncio.gather(*[self.process_sub_query(sub_query) for sub_query in sub_queries])
        print("ãƒ‡ãƒãƒƒã‚° get_context_by_searché–¢æ•° å¾Œ3")
        return context

    async def process_sub_query(self, sub_query: str):
        """Takes in a sub query and scrapes urls based on it and gathers context.

        Args:
            sub_query (str): The sub-query generated from the original query

        Returns:
            str: The context gathered from search
        """
        print("ãƒ‡ãƒãƒƒã‚° process_sub_queryé–¢æ•° å‰1")
        if self.verbose:
            await stream_output("logs", f"\nğŸ” Running research for '{sub_query}'...", self.websocket)
        print("ãƒ‡ãƒãƒƒã‚° process_sub_queryé–¢æ•° å¾Œ1")

        print("ãƒ‡ãƒãƒƒã‚° process_sub_queryé–¢æ•° å‰2")
        scraped_sites = await self.scrape_sites_by_query(sub_query)   # scraped_sites ã¯ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„(è¾æ›¸)ã®ãƒªã‚¹ãƒˆã€‚[content1, content2, content3 â€¦]
        content = await self.get_similar_content_by_query(sub_query, scraped_sites)
        print("ãƒ‡ãƒãƒƒã‚° process_sub_queryé–¢æ•° å¾Œ2")

        if content and self.verbose:
            await stream_output("logs", f"ğŸ“ƒ {content}", self.websocket)
        elif self.verbose:
            await stream_output("logs", f"ğŸ¤· No content found for '{sub_query}'...", self.websocket)
        return content

    async def get_new_urls(self, url_set_input):
        """ Gets the new urls from the given url set.
        Args: url_set_input (set[str]): The url set to get the new urls from
        Returns: list[str]: The new urls from the given url set
        """

        new_urls = []
        for url in url_set_input:
            if url not in self.visited_urls:
                self.visited_urls.add(url)
                new_urls.append(url)
                if self.verbose:
                    await stream_output("logs", f"âœ… Added source url to research: {url}\n", self.websocket)

        return new_urls

    async def scrape_sites_by_query(self, sub_query):
        """
        Runs a sub-query
        Args:
            sub_query:

        Returns:
            scraped_content_results:
                è¾æ›¸ã®ãƒªã‚¹ãƒˆã€‚[{"url": link1, "raw_content": content1}, {"url": link2, "raw_content": content2}, {"url": link3, "raw_content": content3}, â€¦]
                raw_content ã¯ url ã‹ã‚‰ BeautifulSoup ã§å–å¾—ã—ãŸã‚¯ãƒªãƒ¼ãƒ³ãªæ–‡å­—åˆ—(ã‚µãƒãƒªãƒ¼ã§ã¯ãªãWebãƒšãƒ¼ã‚¸ã®ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã€‚ã“ã‚Œã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³åŒ–ã—ã¦ã„ãªã„ã€‚)ã€‚
                â†’ JavaScriptã®ã‚ˆã†ãªå‹•çš„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ BeautifulSoup ã˜ã‚ƒå–å¾—ã§ããªã„æ°—ãŒã™ã‚‹ã®ã§ã€search_results["body"] ã®æ–¹ã‚’ä½¿ã£ãŸæ–¹ãŒè‰¯ã„ã‹ã‚‚ã€‚
        """
        # Get Urls
        retriever = self.retriever(sub_query)
        # ãƒšã‚¬ã‚µã‚¹ã§ search_results["body"] ã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã—ãŸã‘ã©ã€ä½¿ã£ã¦ãªã„ã£ã½ã„ã€‚search_results["href"] ã¯ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã® URL ã®ãƒªã‚¹ãƒˆ
        # search_results["href"] ã§ RAG ã™ã‚‹ãŸã‚ã«ã€åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ query ã«é–¢ä¿‚ã®ã‚ã‚Šãã†ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¦ã„ã‚‹ã€‚
        search_results = retriever.search(
            max_results=self.cfg.max_search_results_per_query)
        new_search_urls = await self.get_new_urls([url.get("href") for url in search_results])

        # Scrape Urls
        if self.verbose:
            await stream_output("logs", f"ğŸ¤” Researching for relevant information...\n", self.websocket)
        # scraped_content_results ã¯ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„(è¾æ›¸)ã®ãƒªã‚¹ãƒˆã€‚[content1, content2, content3 â€¦]
        # content1 ã¯ {"url": link, "raw_content": content} ã¨ã„ã†è¾æ›¸ã§ã€url ã¯ new_search_urls ã®ã†ã¡ã®1ç•ªç›®ã€‚content ã¯ ãã® url ã‹ã‚‰ BeautifulSoup ã§å–å¾—ã—ãŸã‚¯ãƒªãƒ¼ãƒ³ãªæ–‡å­—åˆ—ã§ã€ã‚µãƒãƒªãƒ¼ã˜ã‚ƒãªã„ã—ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³åŒ–ã—ãŸã‚‚ã®ã§ã‚‚ãªãã€Webãƒšãƒ¼ã‚¸ã®ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã€‚
        scraped_content_results = scrape_urls(new_search_urls, self.cfg)
        return scraped_content_results

    async def get_similar_content_by_query(self, query, pages):
        """
        åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ query ã«é–¢ä¿‚ã®ã‚ã‚Šãã†ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¦ã„ã‚‹ã€‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å…¨éƒ¨ã‚’ä½¿ã£ã¦ã„ã‚‹è¨³ã§ã¯ç„¡ã„
        pages ã¯ BeautifulSoup ã§å–å¾—ã—ãŸã‚¯ãƒªãƒ¼ãƒ³ãªæ–‡å­—åˆ—ã§ã‚µãƒãƒªãƒ¼ã˜ã‚ƒãªã„ã—ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³åŒ–ã—ãŸã‚‚ã®ã§ã‚‚ãªãã€Webãƒšãƒ¼ã‚¸ã®ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã€‚
        """
        if self.verbose:
            await stream_output("logs", f"ğŸ“ Getting relevant content based on query: {query}...", self.websocket)
        # Summarize Raw Data
        context_compressor = ContextCompressor(
            documents=pages, embeddings=self.memory.get_embeddings())   # multilingual-e5-XXX ã‚’ä½¿ã£ã¦ã„ã‚‹
        # Run Tasks
        return context_compressor.get_context(query, max_results=8)

    ########################################################################################

    # DETAILED REPORT

    async def write_introduction(self):
        # æœ¬é¡Œèª¿æŸ»ã‹ã‚‰å ±å‘Šæ›¸åºæ–‡ã‚’æ§‹æˆã™ã‚‹
        introduction = await get_report_introduction(self.query, self.context, self.role, self.cfg, self.websocket)

        return introduction

    async def get_subtopics(self):
        """
        This async function generates subtopics based on user input and other parameters.

        Returns:
          The `get_subtopics` function is returning the `subtopics` that are generated by the
        `construct_subtopics` function.
        """
        if self.verbose:
            await stream_output("logs", f"ğŸ¤” Generating subtopics...", self.websocket)

        subtopics = await construct_subtopics(
            task=self.query,
            data=self.context,
            config=self.cfg,
            # This is a list of user provided subtopics
            subtopics=self.subtopics,
        )

        if self.verbose:
            await stream_output("logs", f"ğŸ“‹Subtopics: {subtopics}", self.websocket)

        return subtopics
